{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82cb82cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:List of CSV files in GCS bucket 'gs://ayush_landing_storage/': ['gs://ayush_landing_storage/sales_data_2024-05-31.csv']\n",
      "INFO:__main__:Dynamically created statement: \n",
      "            SELECT file_name\n",
      "            FROM `retail_dataset.product_staging_table`\n",
      "            WHERE file_name = 'sales_data_2024-05-31.csv_2024-05-30.csv' AND status='A'\n",
      "        \n",
      "INFO:__main__:File 'sales_data_2024-05-31.csv_2024-05-30.csv' not found or does not have status 'A' in product_staging_table. Proceeding to next steps.\n",
      "INFO:__main__:Schema for gs://ayush_landing_storage/sales_data_2024-05-31.csv: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']\n",
      "INFO:__main__:Mandatory columns schema: ['customer_id', 'store_id', 'product_name', 'sales_date', 'sales_person_id', 'price', 'quantity', 'total_cost']\n",
      "INFO:__main__:Missing columns: set()\n",
      "INFO:__main__:No schema mismatch found\n",
      "INFO:__main__:**** Inserting today's file with status in the product_staging_table ****\n",
      "INFO:__main__:** Insert statements created for staging table: [\"\\n                       Insert into retail_dataset.product_staging_table (file_name,file_location,created_date,status) \\n                       values ('sales_data_2024-05-31.csv','gs://ayush_landing_storage//sales_data_2024-05-31.csv','2024-06-01 10:04:29','A')\"] **\n",
      "INFO:__main__:**** Staging table insertion  successfull ****\n",
      "INFO:__main__:List of correct files: ['gs://ayush_landing_storage/sales_data_2024-05-31.csv']\n",
      "INFO:__main__:List of error files: []\n",
      "INFO:__main__:****  Fixing extra columns coming from source  ****\n",
      "INFO:__main__:**** Creating empty dataframe ****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------+----------+---------------+-----+--------+----------+-----------------+\n",
      "|customer_id|store_id|product_name|sales_date|sales_person_id|price|quantity|total_cost|additional_column|\n",
      "+-----------+--------+------------+----------+---------------+-----+--------+----------+-----------------+\n",
      "+-----------+--------+------------+----------+---------------+-----+--------+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extra columns present at source []                                \n",
      "INFO:__main__:***** Final data generated  which will be used for  further processing ****\n",
      "INFO:__main__:Load data from a BigQuery table into a Spark DataFrame.           \n",
      "INFO:__main__:*** Loading retail_dataset.customer data into DataFrame ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------+----------+---------------+-----+--------+----------+-----------------+\n",
      "|customer_id|store_id|product_name|sales_date|sales_person_id|price|quantity|total_cost|additional_column|\n",
      "+-----------+--------+------------+----------+---------------+-----+--------+----------+-----------------+\n",
      "|          6|     122| clinic plus|2023-07-30|              5|  1.5|       2|       3.0|             null|\n",
      "|          8|     121|       maida|2023-06-20|              3| 20.0|       9|     180.0|             null|\n",
      "|         14|     123|       sugar|2023-06-30|              7| 50.0|       2|     100.0|             null|\n",
      "|         15|     123|   dantkanti|2023-04-22|              8|100.0|       6|     600.0|             null|\n",
      "|         20|     121| quaker oats|2023-03-09|              1|212.0|       6|    1272.0|             null|\n",
      "|         14|     121|       maida|2023-05-01|              1| 20.0|       7|     140.0|             null|\n",
      "|         19|     122| refined oil|2023-04-27|              4|110.0|       4|     440.0|             null|\n",
      "|         20|     122| refined oil|2023-04-27|              6|110.0|       4|     440.0|             null|\n",
      "|         12|     123| refined oil|2023-03-17|              9|110.0|       8|     880.0|             null|\n",
      "|         10|     121|       sugar|2023-03-11|              3| 50.0|       5|     250.0|             null|\n",
      "|          1|     122|       besan|2023-07-23|              4| 52.0|       6|     312.0|             null|\n",
      "|          2|     122| quaker oats|2023-06-09|              4|212.0|       9|    1908.0|             null|\n",
      "|         18|     123|       sugar|2023-08-03|              7| 50.0|      10|     500.0|             null|\n",
      "|          3|     123|       besan|2023-05-01|              7| 52.0|       8|     416.0|             null|\n",
      "|         20|     121|       sugar|2023-06-07|              3| 50.0|       8|     400.0|             null|\n",
      "|         14|     121|       sugar|2023-06-18|              1| 50.0|       2|     100.0|             null|\n",
      "|         16|     123|       besan|2023-06-05|              8| 52.0|      10|     520.0|             null|\n",
      "|         20|     122| quaker oats|2023-05-25|              6|212.0|       8|    1696.0|             null|\n",
      "|          6|     122|       besan|2023-08-02|              5| 52.0|       8|     416.0|             null|\n",
      "|         14|     121|   dantkanti|2023-07-26|              2|100.0|       6|     600.0|             null|\n",
      "+-----------+--------+------------+----------+---------------+-----+--------+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Loading retail_dataset.product data into DataFrame ***\n",
      "INFO:__main__:*** Loading retail_dataset.product_staging_table data into DataFrame ***\n",
      "INFO:__main__:*** Loading retail_dataset.sales_team data into DataFrame ***\n",
      "INFO:__main__:*** Loading retail_dataset.store data into DataFrame ***\n",
      "INFO:__main__:****  Final enriched  data ****\n",
      "INFO:__main__:Joining the final_df_to_process with customer_table_df \n",
      "INFO:__main__:Joining the customer_df_join with store_table_df                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------+----------+-----------+----------+---------+-------+-------+------------+\n",
      "|customer_id|store_id|sales_date|sales_person_id|total_cost|customer_id|first_name|last_name|address|pincode|phone_number|\n",
      "+-----------+--------+----------+---------------+----------+-----------+----------+---------+-------+-------+------------+\n",
      "|          6|     122|2023-07-30|              5|       3.0|          6|     Romil|  Shanker|  Delhi| 122009|  9129451313|\n",
      "|          8|     121|2023-06-20|              3|     180.0|          8|     Divij|    Garde|  Delhi| 122009|  9141984713|\n",
      "|         14|     123|2023-06-30|              7|     100.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|\n",
      "|         15|     123|2023-04-22|              8|     600.0|         15|     Sahil|Sabharwal|  Delhi| 122009|  9174928780|\n",
      "|         20|     121|2023-03-09|              1|    1272.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|\n",
      "|         14|     121|2023-05-01|              1|     140.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|\n",
      "|         19|     122|2023-04-27|              4|     440.0|         19|  Indranil|    Dutta|  Delhi| 122009|  9120667755|\n",
      "|         20|     122|2023-04-27|              6|     440.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|\n",
      "|         12|     123|2023-03-17|              9|     880.0|         12|     Sumer|   Mangal|  Delhi| 122009|  9138607933|\n",
      "|         10|     121|2023-03-11|              3|     250.0|         10|    Saanvi|  Krishna|  Delhi| 122009|  9173121081|\n",
      "|          1|     122|2023-07-23|              4|     312.0|          1|   Dhanush|    Sahni|  Delhi| 122009|  9155328165|\n",
      "|          2|     122|2023-06-09|              4|    1908.0|          2|    Yasmin|     Shan|  Delhi| 122009|  9191478300|\n",
      "|         18|     123|2023-08-03|              7|     500.0|         18| Vardaniya|     Jani|  Delhi| 122009|  9125068977|\n",
      "|          3|     123|2023-05-01|              7|     416.0|          3|     Vidur|   Mammen|  Delhi| 122009|  9119017511|\n",
      "|         20|     121|2023-06-07|              3|     400.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|\n",
      "|         14|     121|2023-06-18|              1|     100.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|\n",
      "|         16|     123|2023-06-05|              8|     520.0|         16|      Tiya|  Kashyap|  Delhi| 122009|  9105126094|\n",
      "|         20|     122|2023-05-25|              6|    1696.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|\n",
      "|          6|     122|2023-08-02|              5|     416.0|          6|     Romil|  Shanker|  Delhi| 122009|  9129451313|\n",
      "|         14|     121|2023-07-26|              2|     600.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|\n",
      "+-----------+--------+----------+---------------+----------+-----------+----------+---------+-------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Joining the customer_store_df_join with sales_team_table_df       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------+----------+-----------+----------+---------+-------+-------+------------+-------+------------------+\n",
      "|customer_id|store_id|sales_date|sales_person_id|total_cost|customer_id|first_name|last_name|address|pincode|phone_number|address|store_manager_name|\n",
      "+-----------+--------+----------+---------------+----------+-----------+----------+---------+-------+-------+------------+-------+------------------+\n",
      "|          6|     122|2023-07-30|              5|       3.0|          6|     Romil|  Shanker|  Delhi| 122009|  9129451313|  Delhi|            Nikita|\n",
      "|          8|     121|2023-06-20|              3|     180.0|          8|     Divij|    Garde|  Delhi| 122009|  9141984713|  Delhi|            Manish|\n",
      "|         14|     123|2023-06-30|              7|     100.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|  Delhi|            vikash|\n",
      "|         15|     123|2023-04-22|              8|     600.0|         15|     Sahil|Sabharwal|  Delhi| 122009|  9174928780|  Delhi|            vikash|\n",
      "|         20|     121|2023-03-09|              1|    1272.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|  Delhi|            Manish|\n",
      "|         14|     121|2023-05-01|              1|     140.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|  Delhi|            Manish|\n",
      "|         19|     122|2023-04-27|              4|     440.0|         19|  Indranil|    Dutta|  Delhi| 122009|  9120667755|  Delhi|            Nikita|\n",
      "|         20|     122|2023-04-27|              6|     440.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|  Delhi|            Nikita|\n",
      "|         12|     123|2023-03-17|              9|     880.0|         12|     Sumer|   Mangal|  Delhi| 122009|  9138607933|  Delhi|            vikash|\n",
      "|         10|     121|2023-03-11|              3|     250.0|         10|    Saanvi|  Krishna|  Delhi| 122009|  9173121081|  Delhi|            Manish|\n",
      "|          1|     122|2023-07-23|              4|     312.0|          1|   Dhanush|    Sahni|  Delhi| 122009|  9155328165|  Delhi|            Nikita|\n",
      "|          2|     122|2023-06-09|              4|    1908.0|          2|    Yasmin|     Shan|  Delhi| 122009|  9191478300|  Delhi|            Nikita|\n",
      "|         18|     123|2023-08-03|              7|     500.0|         18| Vardaniya|     Jani|  Delhi| 122009|  9125068977|  Delhi|            vikash|\n",
      "|          3|     123|2023-05-01|              7|     416.0|          3|     Vidur|   Mammen|  Delhi| 122009|  9119017511|  Delhi|            vikash|\n",
      "|         20|     121|2023-06-07|              3|     400.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|  Delhi|            Manish|\n",
      "|         14|     121|2023-06-18|              1|     100.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|  Delhi|            Manish|\n",
      "|         16|     123|2023-06-05|              8|     520.0|         16|      Tiya|  Kashyap|  Delhi| 122009|  9105126094|  Delhi|            vikash|\n",
      "|         20|     122|2023-05-25|              6|    1696.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|  Delhi|            Nikita|\n",
      "|          6|     122|2023-08-02|              5|     416.0|          6|     Romil|  Shanker|  Delhi| 122009|  9129451313|  Delhi|            Nikita|\n",
      "|         14|     121|2023-07-26|              2|     600.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|  Delhi|            Manish|\n",
      "+-----------+--------+----------+---------------+----------+-----------+----------+---------+-------+-------+------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Preparing data for customer data mart ***                     \n",
      "INFO:__main__:*** Calculating customer every month puchased amount ****\n",
      "INFO:__main__:Final customer_data_mart data to be loadings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------------+----------+-----------+----------+---------+-------+-------+------------+-------+------------------+----------+---------+----------+----------+-------+-------+------------+-----------------------+----------------------+--------------------+--------------------+\n",
      "|customer_id|store_id|sales_date|sales_person_id|total_cost|customer_id|first_name|last_name|address|pincode|phone_number|address|store_manager_name|first_name|last_name|manager_id|is_manager|address|pincode|joining_date|sales_person_first_name|sales_person_last_name|sales_person_address|sales_person_pincode|\n",
      "+-----------+--------+----------+---------------+----------+-----------+----------+---------+-------+-------+------------+-------+------------------+----------+---------+----------+----------+-------+-------+------------+-----------------------+----------------------+--------------------+--------------------+\n",
      "|          6|     122|2023-07-30|              5|       3.0|          6|     Romil|  Shanker|  Delhi| 122009|  9129451313|  Delhi|            Nikita|      Neha|    Kumar|        10|         N|  Delhi| 122007|  2020-05-01|                   Neha|                 Kumar|               Delhi|              122007|\n",
      "|          8|     121|2023-06-20|              3|     180.0|          8|     Divij|    Garde|  Delhi| 122009|  9141984713|  Delhi|            Manish|      Amit|   Sharma|        10|         N|  Delhi| 122007|  2020-05-01|                   Amit|                Sharma|               Delhi|              122007|\n",
      "|         14|     123|2023-06-30|              7|     100.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|  Delhi|            vikash|     Anita| Malhotra|        10|         N|  Delhi| 122007|  2020-05-01|                  Anita|              Malhotra|               Delhi|              122007|\n",
      "|         15|     123|2023-04-22|              8|     600.0|         15|     Sahil|Sabharwal|  Delhi| 122009|  9174928780|  Delhi|            vikash|      Alok|   Rajput|        10|         N|  Delhi| 122007|  2020-05-01|                   Alok|                Rajput|               Delhi|              122007|\n",
      "|         20|     121|2023-03-09|              1|    1272.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|  Delhi|            Manish|     Rahul|    Verma|        10|         N|  Delhi| 122007|  2020-05-01|                  Rahul|                 Verma|               Delhi|              122007|\n",
      "|         14|     121|2023-05-01|              1|     140.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|  Delhi|            Manish|     Rahul|    Verma|        10|         N|  Delhi| 122007|  2020-05-01|                  Rahul|                 Verma|               Delhi|              122007|\n",
      "|         19|     122|2023-04-27|              4|     440.0|         19|  Indranil|    Dutta|  Delhi| 122009|  9120667755|  Delhi|            Nikita|     Sneha|    Gupta|        10|         N|  Delhi| 122007|  2020-05-01|                  Sneha|                 Gupta|               Delhi|              122007|\n",
      "|         20|     122|2023-04-27|              6|     440.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|  Delhi|            Nikita|     Vijay|    Yadav|        10|         N|  Delhi| 122007|  2020-05-01|                  Vijay|                 Yadav|               Delhi|              122007|\n",
      "|         12|     123|2023-03-17|              9|     880.0|         12|     Sumer|   Mangal|  Delhi| 122009|  9138607933|  Delhi|            vikash|    Monica|     Jain|        10|         N|  Delhi| 122007|  2020-05-01|                 Monica|                  Jain|               Delhi|              122007|\n",
      "|         10|     121|2023-03-11|              3|     250.0|         10|    Saanvi|  Krishna|  Delhi| 122009|  9173121081|  Delhi|            Manish|      Amit|   Sharma|        10|         N|  Delhi| 122007|  2020-05-01|                   Amit|                Sharma|               Delhi|              122007|\n",
      "|          1|     122|2023-07-23|              4|     312.0|          1|   Dhanush|    Sahni|  Delhi| 122009|  9155328165|  Delhi|            Nikita|     Sneha|    Gupta|        10|         N|  Delhi| 122007|  2020-05-01|                  Sneha|                 Gupta|               Delhi|              122007|\n",
      "|          2|     122|2023-06-09|              4|    1908.0|          2|    Yasmin|     Shan|  Delhi| 122009|  9191478300|  Delhi|            Nikita|     Sneha|    Gupta|        10|         N|  Delhi| 122007|  2020-05-01|                  Sneha|                 Gupta|               Delhi|              122007|\n",
      "|         18|     123|2023-08-03|              7|     500.0|         18| Vardaniya|     Jani|  Delhi| 122009|  9125068977|  Delhi|            vikash|     Anita| Malhotra|        10|         N|  Delhi| 122007|  2020-05-01|                  Anita|              Malhotra|               Delhi|              122007|\n",
      "|          3|     123|2023-05-01|              7|     416.0|          3|     Vidur|   Mammen|  Delhi| 122009|  9119017511|  Delhi|            vikash|     Anita| Malhotra|        10|         N|  Delhi| 122007|  2020-05-01|                  Anita|              Malhotra|               Delhi|              122007|\n",
      "|         20|     121|2023-06-07|              3|     400.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|  Delhi|            Manish|      Amit|   Sharma|        10|         N|  Delhi| 122007|  2020-05-01|                   Amit|                Sharma|               Delhi|              122007|\n",
      "|         14|     121|2023-06-18|              1|     100.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|  Delhi|            Manish|     Rahul|    Verma|        10|         N|  Delhi| 122007|  2020-05-01|                  Rahul|                 Verma|               Delhi|              122007|\n",
      "|         16|     123|2023-06-05|              8|     520.0|         16|      Tiya|  Kashyap|  Delhi| 122009|  9105126094|  Delhi|            vikash|      Alok|   Rajput|        10|         N|  Delhi| 122007|  2020-05-01|                   Alok|                Rajput|               Delhi|              122007|\n",
      "|         20|     122|2023-05-25|              6|    1696.0|         20|     Kavya|   Sachar|  Delhi| 122009|  9157628717|  Delhi|            Nikita|     Vijay|    Yadav|        10|         N|  Delhi| 122007|  2020-05-01|                  Vijay|                 Yadav|               Delhi|              122007|\n",
      "|          6|     122|2023-08-02|              5|     416.0|          6|     Romil|  Shanker|  Delhi| 122009|  9129451313|  Delhi|            Nikita|      Neha|    Kumar|        10|         N|  Delhi| 122007|  2020-05-01|                   Neha|                 Kumar|               Delhi|              122007|\n",
      "|         14|     121|2023-07-26|              2|     600.0|         14|    Yuvaan|     Bawa|  Delhi| 122009|  9162077019|  Delhi|            Manish|     Priya|    Singh|        10|         N|  Delhi| 122007|  2020-05-01|                  Priya|                 Singh|               Delhi|              122007|\n",
      "+-----------+--------+----------+---------------+----------+-----------+----------+---------+-------+-------+------------+-------+------------------+----------+---------+----------+----------+-------+-------+------------+-----------------------+----------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Final data for customer_data_mart ***                         \n",
      "INFO:__main__:*** Writing data to BigQuery table retail_dataset.customers_data_mart ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------+------------+-------------------------+-----------+\n",
      "|customer_id|      full_name|address|phone_number|to_date(sales_date_month)|total_sales|\n",
      "+-----------+---------------+-------+------------+-------------------------+-----------+\n",
      "|         10| Saanvi Krishna|  Delhi|  9173121081|               2023-07-01|  1820901.0|\n",
      "|         19| Indranil Dutta|  Delhi|  9120667755|               2023-03-01|  1708639.0|\n",
      "|         13|   Rhea Chander|  Delhi|  9103434731|               2023-08-01|  1201057.0|\n",
      "|         14|    Yuvaan Bawa|  Delhi|  9162077019|               2023-08-01|  1195927.0|\n",
      "|         19| Indranil Dutta|  Delhi|  9120667755|               2023-04-01|  1761097.5|\n",
      "|          3|   Vidur Mammen|  Delhi|  9119017511|               2023-06-01|  1777512.0|\n",
      "|         20|   Kavya Sachar|  Delhi|  9157628717|               2023-03-01|  1741902.5|\n",
      "|         19| Indranil Dutta|  Delhi|  9120667755|               2023-07-01|  1800073.0|\n",
      "|         20|   Kavya Sachar|  Delhi|  9157628717|               2023-07-01|  1799379.0|\n",
      "|         11|  Zara Dhaliwal|  Delhi|  9129776379|               2023-08-01|  1202571.0|\n",
      "|         13|   Rhea Chander|  Delhi|  9103434731|               2023-06-01|  1737384.5|\n",
      "|          8|    Divij Garde|  Delhi|  9141984713|               2023-05-01|  1811551.5|\n",
      "|         20|   Kavya Sachar|  Delhi|  9157628717|               2023-05-01|  1794715.5|\n",
      "|         17|    Kimaya Lala|  Delhi|  9115616831|               2023-03-01|  1659845.5|\n",
      "|          4|  Shamik Doctor|  Delhi|  9105180499|               2023-07-01|  1856487.5|\n",
      "|         12|   Sumer Mangal|  Delhi|  9138607933|               2023-03-01|  1738336.5|\n",
      "|         11|  Zara Dhaliwal|  Delhi|  9129776379|               2023-07-01|  1880836.0|\n",
      "|         15|Sahil Sabharwal|  Delhi|  9174928780|               2023-07-01|  1810676.5|\n",
      "|          6|  Romil Shanker|  Delhi|  9129451313|               2023-03-01|  1725022.0|\n",
      "|          9|     Hunar Tank|  Delhi|  9169808085|               2023-06-01|  1738940.0|\n",
      "+-----------+---------------+-------+------------+-------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:**** Data written to BigQuery table retail_dataset.customers_data_mart ****\n",
      "INFO:__main__:*** Writing data to gs://processeddataretail/Customer/ in Parquet format ***\n",
      "INFO:__main__:**** Data written to local disk at path gs://processeddataretail/Customer/ ****\n",
      "INFO:__main__:*** Preparing data for sales data mart ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sales_data_mart data to be loadings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Writing data to BigQuery table retail_dataset.sales_team_data_mart ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------------+-----------+-----------+---------+\n",
      "|store_id|sales_person_id|     full_name|sales_month|total_sales|incentive|\n",
      "+--------+---------------+--------------+-----------+-----------+---------+\n",
      "|     121|              1|   Rahul Verma|    2023-03|  3799235.0| 37992.35|\n",
      "|     121|              2|   Priya Singh|    2023-03|  3886106.5|      0.0|\n",
      "|     121|              3|   Amit Sharma|    2023-03|  3900652.0|      0.0|\n",
      "|     121|              1|   Rahul Verma|    2023-04|  3785396.5| 37853.97|\n",
      "|     121|              3|   Amit Sharma|    2023-04|  3920749.0|      0.0|\n",
      "|     121|              2|   Priya Singh|    2023-04|  3927510.5|      0.0|\n",
      "|     121|              2|   Priya Singh|    2023-05|  4027923.5| 40279.24|\n",
      "|     121|              1|   Rahul Verma|    2023-05|  4118558.0|      0.0|\n",
      "|     121|              3|   Amit Sharma|    2023-05|  4124791.0|      0.0|\n",
      "|     122|              4|   Sneha Gupta|    2023-07|  3983610.5| 39836.11|\n",
      "|     122|              6|   Vijay Yadav|    2023-07|  4064662.0|      0.0|\n",
      "|     122|              5|    Neha Kumar|    2023-07|  4092984.0|      0.0|\n",
      "|     121|              1|   Rahul Verma|    2023-07|  4039388.5| 40393.89|\n",
      "|     121|              3|   Amit Sharma|    2023-07|  4040677.5|      0.0|\n",
      "|     121|              2|   Priya Singh|    2023-07|  4050350.5|      0.0|\n",
      "|     121|              3|   Amit Sharma|    2023-08|  2563522.5| 25635.23|\n",
      "|     121|              2|   Priya Singh|    2023-08|  2583248.0|      0.0|\n",
      "|     121|              1|   Rahul Verma|    2023-08|  2670014.5|      0.0|\n",
      "|     123|              9|   Monica Jain|    2023-03|  3753917.0| 37539.17|\n",
      "|     123|              7|Anita Malhotra|    2023-03|  3819563.5|      0.0|\n",
      "+--------+---------------+--------------+-----------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:**** Data written to BigQuery table retail_dataset.sales_team_data_mart ****\n",
      "INFO:__main__:*** Writing data to gs://processeddataretail/Sales/ in Parquet format ***\n",
      "INFO:__main__:**** Data written to local disk at path gs://processeddataretail/Sales/ ****\n",
      "INFO:__main__:**** Updating today's file with status in the product_staging_table ****\n",
      "INFO:__main__:** Update statements created for staging table: [\"\\n            UPDATE `retail_dataset.product_staging_table`\\n            SET status = 'I', updated_date = TIMESTAMP '2024-06-01 10:04:29'\\n            WHERE file_name = 'sales_data_2024-05-31.csv'\\n        \"] **\n",
      "INFO:__main__:**** Staging table updation  successfull ****\n",
      "INFO:__main__:Process completed.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery, storage\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "def list_csv_files_in_gcs(bucket_name, directory_path=''):\n",
    "    \"\"\"\n",
    "    List all CSV files in a GCS bucket under the specified directory path.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blobs = bucket.list_blobs(prefix=directory_path)\n",
    "        csv_files = [f\"gs://{bucket_name}/{blob.name}\" for blob in blobs if blob.name.endswith('.csv')]\n",
    "        return csv_files\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching files from GCS bucket: {e}\")\n",
    "        return []\n",
    "\n",
    "def check_files_status_in_bigquery(file_name):\n",
    "    \"\"\" \n",
    "    Check if the previous day's file exists in BigQuery table 'product_staging_table' with status='A'.\n",
    "    If yes, abort the script; if no, insert today's file and proceed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get previous day's date\n",
    "        file_name = \"sales_data_2024-05-31.csv\"\n",
    "        filename_today_date = file_name[11:21]  # Extract \"2024-05-31\" as a string\n",
    "        filename_date = datetime.datetime.strptime(filename_today_date, \"%Y-%m-%d\").date()  # Convert string to datetime.date\n",
    "\n",
    "        yesterday = filename_date - datetime.timedelta(days=1)\n",
    "        previous_day_date = yesterday.strftime(\"%Y-%m-%d\")\n",
    "        new_file_name = file_name[:10]\n",
    "        # Check if previous day's file exists with status 'A'\n",
    "        statement = f\"\"\"\n",
    "            SELECT file_name\n",
    "            FROM `{database_name}.{product_staging_table}`\n",
    "            WHERE file_name = '{file_name}_{previous_day_date}.csv' AND status='A'\n",
    "        \"\"\"\n",
    "        logger.info(f\"Dynamically created statement: {statement}\")\n",
    "   \n",
    "        query_job = client.query(statement)\n",
    "        results = query_job.result()\n",
    "\n",
    "        if results.total_rows > 0:\n",
    "            logger.error(f\"Previous day's file '{previous_day_date}.csv' exists in product_staging_table with status 'A'. Aborting further processing.\")\n",
    "            return False\n",
    "        else:\n",
    "            logger.info(f\"File '{file_name}_{previous_day_date}.csv' not found or does not have status 'A' in product_staging_table. Proceeding to next steps.\")\n",
    "            return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error executing BigQuery query: {e}\")\n",
    "        return False\n",
    "\n",
    "        \n",
    "def validate_file_schemas(csv_files):\n",
    "    \"\"\"\n",
    "    Validate schemas of CSV files against mandatory columns.\n",
    "    \"\"\"\n",
    "    correct_files = []\n",
    "    error_files = []\n",
    "\n",
    "    for data in csv_files:\n",
    "        try:\n",
    "            # Here you would typically use Spark or pandas to validate the schema\n",
    "            # This example assumes you are using Spark\n",
    "            data_schema = spark.read.format('csv').option('header', 'true').load(data).columns\n",
    "            logger.info(f\"Schema for {data}: {data_schema}\")\n",
    "            logger.info(f\"Mandatory columns schema: {mandatory_columns}\")\n",
    "            missing_columns = set(mandatory_columns) - set(data_schema)\n",
    "            logger.info(f\"Missing columns: {missing_columns}\")\n",
    "\n",
    "            if missing_columns:\n",
    "                error_files.append(data)\n",
    "                logger.info(\"Schema mismatch found, moving file to error directory.\")\n",
    "            else:\n",
    "                logger.info(\"No schema mismatch found\")\n",
    "                correct_files.append(data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error validating schema for file {data}: {e}\")\n",
    "            error_files.append(data)\n",
    "            continue\n",
    "\n",
    "    return correct_files, error_files\n",
    "\n",
    "def insert_staging_table(filename):\n",
    "    \"\"\"\n",
    "    Insert the file the status in BigQuery table 'product_staging_table' for correct files.\n",
    "    \"\"\"\n",
    "    logger.info(\"**** Inserting today's file with status in the product_staging_table ****\")\n",
    "\n",
    "    if not correct_files:\n",
    "        logger.info(\"** No files to process\")\n",
    "        return\n",
    "\n",
    "    insert_statements = []\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    for file in correct_files:\n",
    "        filename = os.path.basename(file)\n",
    "        full_gcs_path = f'gs://{bucket_name}/{directory_path}/{filename}'\n",
    "        statement = f\"\"\"\n",
    "                       Insert into {database_name}.{product_staging_table} (file_name,file_location,created_date,status) \n",
    "                       values ('{filename}','{full_gcs_path}','{formatted_date}','A')\"\"\"\n",
    "\n",
    "        insert_statements.append(statement)\n",
    "\n",
    "    logger.info(f\"** Insert statements created for staging table: {insert_statements} **\")\n",
    "\n",
    "    try:\n",
    "        for statement in insert_statements:\n",
    "            query_job = client.query(statement)\n",
    "            query_job.result()\n",
    "\n",
    "        logger.info(\"**** Staging table insertion  successfull ****\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inserting in  staging table: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def update_staging_table(filename):\n",
    "    \"\"\"\n",
    "    update the file  status in BigQuery table 'product_staging_table' for correct files once the process completes.\n",
    "    \"\"\"\n",
    "    logger.info(\"**** Updating today's file with status in the product_staging_table ****\")\n",
    "\n",
    "    if not correct_files:\n",
    "        logger.info(\"** No files to process\")\n",
    "        return\n",
    "\n",
    "    update_statements = []\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    for file in correct_files:\n",
    "        filename = os.path.basename(file)\n",
    "        full_gcs_path = f'gs://{bucket_name}/{directory_path}/{filename}'\n",
    "        statement = f\"\"\"\n",
    "            UPDATE `{database_name}.{product_staging_table}`\n",
    "            SET status = 'I', updated_date = TIMESTAMP '{formatted_date}'\n",
    "            WHERE file_name = '{filename}'\n",
    "        \"\"\"\n",
    "\n",
    "        update_statements.append(statement)\n",
    "\n",
    "    logger.info(f\"** Update statements created for staging table: {update_statements} **\")\n",
    "\n",
    "    try:\n",
    "        for statement in update_statements:\n",
    "            query_job = client.query(statement)\n",
    "            query_job.result()\n",
    "\n",
    "        logger.info(\"**** Staging table updation  successfull ****\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating in  staging table: {e}\")\n",
    "        raise\n",
    "\n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "def extra_details_check(filename):\n",
    "    logger.info(\"****  Fixing extra columns coming from source  ****\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"store_id\", IntegerType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"sales_date\", DateType(), True),\n",
    "        StructField(\"sales_person_id\", IntegerType(), True),\n",
    "        StructField(\"price\", FloatType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"total_cost\", FloatType(), True),\n",
    "        StructField(\"additional_column\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "    logger.info(\"**** Creating empty dataframe ****\")\n",
    "\n",
    "    final_df_to_process = spark.createDataFrame([], schema)\n",
    "    final_df_to_process.show()\n",
    "    for data in correct_files:\n",
    "        data_df=spark.read.format('csv').option('header','true').option('inferSchema','true').load(data)\n",
    "        data_schema=data_df.columns\n",
    "        extra_columns=list(set(data_schema)-set(mandatory_columns))\n",
    "        logger.info(f'Extra columns present at source {extra_columns}')\n",
    "        #If there are extra columns it is concatanating all the extra columns seperated by , and placing it in additional column\n",
    "        if extra_columns:\n",
    "            data_df=data_df.withColumn(\"additional_column\",concat_ws(\", \",*extra_columns))\\\n",
    "                .select(\"customer_id\",\"store_id\",\"product_name\",\"sales_date\",\"sales_person_id\",\"price\",\"quantity\",\"total_cost\",\"additional_column\")\n",
    "            logger.info(f\"*** Processed {data}  and added additional  columns  ***\")\n",
    "        # If no extra columns  then  additional columns  insert None as value\n",
    "        else:\n",
    "            data_df=data_df.withColumn(\"additional_column\",lit(None)) \\\n",
    "            .select(\"customer_id\", \"store_id\", \"product_name\", \"sales_date\", \"sales_person_id\", \"price\", \"quantity\",\n",
    "                    \"total_cost\", \"additional_column\")\n",
    "\n",
    "\n",
    "    final_df_to_process=final_df_to_process.union(data_df)\n",
    "    \n",
    "    return final_df_to_process\n",
    "\n",
    "    \n",
    "def load_table_into_dataframe(dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Load data from a BigQuery table into a Spark DataFrame.\n",
    "    :param dataset_name: BigQuery dataset name\n",
    "    :param table_name: BigQuery table name\n",
    "    :return: DataFrame with the table data\n",
    "    \"\"\"\n",
    "    full_table_name = f\"{dataset_name}.{table_name}\"\n",
    "    logger.info(f\"*** Loading {full_table_name} data into DataFrame ***\")\n",
    "    \n",
    "    df = spark.read.format(\"bigquery\") \\\n",
    "        .option(\"dataset\", dataset_name) \\\n",
    "        .option(\"table\", table_name) \\\n",
    "        .load()\n",
    "    \n",
    "    return df\n",
    "\n",
    "#enriching the data from different table\n",
    "def dimesions_table_join(final_df_to_process,\n",
    "                         customer_table_df,store_table_df,sales_team_table_df):\n",
    "    logger.info(\"Joining the final_df_to_process with customer_table_df \")\n",
    "    customer_df_join = final_df_to_process.alias(\"s3_data\") \\\n",
    "        .join(customer_table_df.alias(\"ct\"),\n",
    "              col(\"s3_data.customer_id\") == col(\"ct.customer_id\"),\"inner\") \\\n",
    "        .drop(\"product_name\",\"price\",\"quantity\",\"additional_column\",\n",
    "              \"s3_data.customer_id\",\"customer_joining_date\")\n",
    "    \n",
    "    customer_df_join.show()\n",
    "    logger.info(\"Joining the customer_df_join with store_table_df \")\n",
    "    customer_store_df_join= customer_df_join.join(store_table_df,\n",
    "                             store_table_df[\"id\"]==customer_df_join[\"store_id\"],\n",
    "                             \"inner\")\\\n",
    "                        .drop(\"id\",\"store_pincode\",\"store_opening_date\",\"reviews\")\n",
    "    customer_store_df_join.show()\n",
    "    logger.info(\"Joining the customer_store_df_join with sales_team_table_df \")\n",
    "    customer_store_sales_df_join = customer_store_df_join.join(sales_team_table_df.alias(\"st\"),\n",
    "                             col(\"st.id\")==customer_store_df_join[\"sales_person_id\"],\n",
    "                             \"inner\")\\\n",
    "                .withColumn(\"sales_person_first_name\",col(\"st.first_name\"))\\\n",
    "                .withColumn(\"sales_person_last_name\",col(\"st.last_name\"))\\\n",
    "                .withColumn(\"sales_person_address\",col(\"st.address\"))\\\n",
    "                .withColumn(\"sales_person_pincode\",col(\"st.pincode\"))\\\n",
    "                .drop(\"id\",\"st.first_name\",\"st.last_name\",\"st.address\",\"st.pincode\")\n",
    "\n",
    "    return customer_store_sales_df_join\n",
    "\n",
    "\n",
    "def customer_mart_calculation_table_write(final_customer_data_mart_df):\n",
    "    window = Window.partitionBy(\"customer_id\",\"sales_date_month\")\n",
    "    final_customer_data_mart = final_customer_data_mart_df.withColumn(\"sales_date_month\",\n",
    "                                           substring(col(\"sales_date\"),1,7))\\\n",
    "                    .withColumn(\"total_sales_every_month_by_each_customer\",\n",
    "                                sum(\"total_cost\").over(window))\\\n",
    "                    .select(\"customer_id\", concat(col(\"first_name\"),lit(\" \"),col(\"last_name\"))\n",
    "                            .alias(\"full_name\"),\"address\",\"phone_number\",\n",
    "                            to_date(col(\"sales_date_month\")),\n",
    "                            col(\"total_sales_every_month_by_each_customer\").alias(\"total_sales\"))\\\n",
    "                    .distinct()\n",
    "    logger.info(\"Final customer_data_mart data to be loadings\")\n",
    "    final_customer_data_mart.show()\n",
    "    return final_customer_data_mart\n",
    "\n",
    "\n",
    "def sales_mart_calculation_table_write(final_sales_data_mart_df):\n",
    "    window = Window.partitionBy(\"store_id\",\"sales_person_id\",\"sales_month\")\n",
    "    final_sales_data_mart = final_sales_data_mart_df.withColumn(\"sales_month\",\n",
    "                                           substring(col(\"sales_date\"),1,7))\\\n",
    "                    .withColumn(\"total_sales_every_month\",\n",
    "                                sum(col(\"total_cost\")).over(window))\\\n",
    "                    .select(\"store_id\", \"sales_person_id\",concat(col(\"sales_person_first_name\"),lit(\" \"),col(\"sales_person_last_name\"))\n",
    "                            .alias(\"full_name\"),\n",
    "                            \"sales_month\",\n",
    "                            col(\"total_sales_every_month\"))\\\n",
    "                    .distinct()\n",
    "\n",
    "    rank_window=Window.partitionBy(\"store_id\",\"sales_month\").orderBy(col(\"total_sales_every_month\"))\n",
    "    final_sales_data_mart=final_sales_data_mart.withColumn(\"rnk\",rank().over(rank_window))\\\n",
    "                            .withColumn(\"incentive\", when(col(\"rnk\")==1,col(\"total_sales_every_month\")*0.01)\\\n",
    "                            .otherwise(lit(0)))\\\n",
    "                            .withColumn(\"incentive\",round(col(\"incentive\"),2))\\\n",
    "                             .withColumn(\"total_sales\",col(\"total_sales_every_month\"))\\\n",
    "                            .select(\"store_id\",\"sales_person_id\",\"full_name\",\"sales_month\",\"total_sales\",\"incentive\")#Write the Data into MySQL customers_data_mart table\n",
    "    print(\"Final sales_data_mart data to be loadings\")\n",
    "    final_sales_data_mart.show()\n",
    "    return final_sales_data_mart\n",
    "\n",
    "\n",
    "\n",
    "def write_to_bigquery(dataframe, database_name, table_name,temp_gcs_bucket ,mode=\"overwrite\"):\n",
    "    \"\"\"\n",
    "    Write a Spark DataFrame to a BigQuery table.\n",
    "    \n",
    "    :param dataframe: Spark DataFrame to write\n",
    "    :param project_id: Google Cloud project ID\n",
    "    :param dataset_name: BigQuery dataset name\n",
    "    :param table_name: BigQuery table name\n",
    "    :param mode: Write mode (default is 'overwrite')\n",
    "    \"\"\"\n",
    "    full_table_name = f\"{database_name}.{table_name}\"\n",
    "    logger.info(f\"*** Writing data to BigQuery table {full_table_name} ***\")\n",
    "    \n",
    "    dataframe.write.format(\"bigquery\") \\\n",
    "        .option(\"table\", full_table_name) \\\n",
    "        .option(\"temporaryGcsBucket\", temp_gcs_bucket) \\\n",
    "        .mode(mode) \\\n",
    "        .save()\n",
    "\n",
    "    logger.info(f\"**** Data written to BigQuery table {full_table_name} ****\")\n",
    "\n",
    "\n",
    "\n",
    "def write_to_parquet(dataframe, path, partition):\n",
    "    \"\"\"\n",
    "    Write a Spark DataFrame to a Parquet file.\n",
    "    \n",
    "    :param dataframe: Spark DataFrame to write\n",
    "    :param path: Path where the Parquet file will be written\n",
    "    :param mode: Write mode (default is 'overwrite')\n",
    "    \"\"\"\n",
    "    logger.info(f\"*** Writing data to {path} in Parquet format ***\")\n",
    "    if partition==1:\n",
    "        \n",
    "        dataframe.write.format(\"parquet\")\\\n",
    "                .option(\"header\",\"true\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .partitionBy(\"sales_month\",\"store_id\")\\\n",
    "                .option(\"path\",path)\\\n",
    "                .save()\n",
    "        logger.info(f\"**** Data written to local disk at path {path} ****\")\n",
    "    \n",
    "    else:\n",
    "        dataframe.write.format(\"parquet\")\\\n",
    "                .option(\"header\",\"true\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .option(\"path\",path)\\\n",
    "                .save()  \n",
    "        logger.info(f\"**** Data written to local disk at path {path} ****\")\n",
    "    \n",
    "    \n",
    "def process_customer_data_mart(customer_store_sales_df_join, output_path,database_name, table_name,temp_gcs_bucket):\n",
    "    \"\"\"\n",
    "    Process and write customer data mart DataFrame to Parquet.\n",
    "    :param s3_customer_store_sales_df_join: DataFrame containing joined customer, store, and sales data\n",
    "    \"\"\"\n",
    "    logger.info(\"*** Preparing data for customer data mart ***\")\n",
    "    final_customer_data_mart_df = customer_store_sales_df_join.select(\n",
    "        \"ct.customer_id\", \"ct.first_name\", \"ct.last_name\", \"ct.address\", \"ct.pincode\", \"phone_number\", \"sales_date\", \"total_cost\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    logger.info(\"*** Calculating customer every month puchased amount ****\")\n",
    "    final_customer_data_mart=customer_mart_calculation_table_write(final_customer_data_mart_df)\n",
    "    logger.info(\"*** Final data for customer_data_mart ***\")\n",
    "    write_to_bigquery(final_customer_data_mart_df,database_name, table_name,temp_gcs_bucket)\n",
    "    write_to_parquet(final_customer_data_mart_df, output_path,0)\n",
    " \n",
    "\n",
    "    \n",
    "def process_sales_data_mart(customer_store_sales_df_join, output_path,database_name, table_name,temp_gcs_bucket):\n",
    "    \"\"\"\n",
    "    Process and write sales data mart DataFrame to Parquet.\n",
    "    \"\"\"\n",
    "    logger.info(\"*** Preparing data for sales data mart ***\")\n",
    "    final_sales_data_mart_df=customer_store_sales_df_join\\\n",
    "                            .select(\"store_id\",\"sales_person_id\",\"sales_person_first_name\"\n",
    "                                    ,\"sales_person_last_name\",\"store_manager_name\"\n",
    "                                    ,\"sales_person_address\",\"sales_person_pincode\",\"sales_date\",\"total_cost\",\n",
    "                                    expr(\"substring(sales_date,1,7) as sales_month\"))\n",
    "    \n",
    "    final_sales_data_mart=sales_mart_calculation_table_write(final_sales_data_mart_df)\n",
    "\n",
    "    write_to_bigquery(final_sales_data_mart,database_name, table_name,temp_gcs_bucket)\n",
    "    write_to_parquet(final_sales_data_mart, output_path,1)\n",
    "    \n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        \n",
    "        spark = SparkSession \\\n",
    "          .builder \\\n",
    "          .master('yarn') \\\n",
    "          .appName('spark-bigquery-demo') \\\n",
    "          .getOrCreate()\n",
    "        # Initialize BigQuery client\n",
    "        bigquery_client = bigquery.Client()\n",
    "\n",
    "        # Initialize logger\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Define constants\n",
    "        database_name = \"retail_dataset\"\n",
    "        product_staging_table = \"product_staging_table\"\n",
    "        mandatory_columns = [\"customer_id\", \"store_id\", \"product_name\", \"sales_date\", \"sales_person_id\", \"price\", \"quantity\", \"total_cost\"]\n",
    "\n",
    "        current_date=datetime.datetime.now()\n",
    "        formatted_date=current_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        # Define GCS bucket and directory path\n",
    "        # Table name\n",
    "        customer_table_name = \"customer\"\n",
    "        product_staging_table = \"product_staging_table\"\n",
    "        product_table = \"product\"\n",
    "        sales_team_table = \"sales_team\"\n",
    "        store_table = \"store\"\n",
    "        customers_data_mart=\"customers_data_mart\"\n",
    "        sales_team_data_mart=\"sales_team_data_mart\"\n",
    "        \n",
    "        file_to_process = 'sales_data.csv'\n",
    "        bucket_name = 'ayush_landing_storage'\n",
    "        directory_path = ''  # Leave empty if you want to list from the root of the bucket\n",
    "        \n",
    "        temp_gcs_bucket=\"tempgscbucket\"\n",
    "        processed_gcs_bucket_name='processeddataretail'\n",
    "        processed_customer_data_mart_folder='Customer'\n",
    "        processed_Sales_data_mart_folder='Sales'\n",
    "        # List CSV files in GCS\n",
    "        csv_files = list_csv_files_in_gcs(bucket_name, directory_path)\n",
    "        logger.info(f\"List of CSV files in GCS bucket 'gs://{bucket_name}/{directory_path}': {csv_files}\")\n",
    "\n",
    "        # Check file statuses in BigQuery\n",
    "        if check_files_status_in_bigquery(file_to_process):\n",
    "            # Validate file schemas\n",
    "            correct_files, error_files = validate_file_schemas(csv_files)\n",
    "\n",
    "            # Update file statuses in BigQuery\n",
    "            insert_staging_table(file_to_process)\n",
    "\n",
    "            logger.info(f\"List of correct files: {correct_files}\")\n",
    "            logger.info(f\"List of error files: {error_files}\")\n",
    "            \n",
    "            #Extra column check\n",
    "            final_df_to_process=extra_details_check(file_to_process)\n",
    "            logger.info(\"***** Final data generated  which will be used for  further processing ****\")\n",
    "            final_df_to_process.show()\n",
    "            #Load data from a BigQuery table into a Spark DataFrame.\n",
    "            logger.info(\"Load data from a BigQuery table into a Spark DataFrame.\")\n",
    "            customer_table_df = load_table_into_dataframe( database_name, customer_table_name)\n",
    "            product_table_df = load_table_into_dataframe( database_name, product_table)\n",
    "            product_staging_table_df = load_table_into_dataframe( database_name, product_staging_table)\n",
    "            sales_team_table_df = load_table_into_dataframe( database_name, sales_team_table)\n",
    "            store_table_df = load_table_into_dataframe( database_name, store_table)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            #Final enrictched  data\n",
    "            logger.info(\"****  Final enriched  data ****\")\n",
    "            customer_store_sales_df_join=dimesions_table_join(final_df_to_process,customer_table_df,store_table_df,sales_team_table_df)\n",
    "            customer_store_sales_df_join.show()\n",
    "\n",
    "            parquet_path_customer = f\"gs://{processed_gcs_bucket_name}/{processed_customer_data_mart_folder}/\"\n",
    "            parquet_path_sales=f\"gs://{processed_gcs_bucket_name}/{processed_Sales_data_mart_folder}/\"\n",
    "            process_customer_data_mart(customer_store_sales_df_join,parquet_path_customer,database_name,customers_data_mart,temp_gcs_bucket)\n",
    "            process_sales_data_mart(customer_store_sales_df_join,parquet_path_sales,database_name,sales_team_data_mart,temp_gcs_bucket)\n",
    "            today = datetime.date.today()\n",
    "            \n",
    "            update_staging_table(file_to_process)\n",
    "            logger.info(\"Process completed.\")\n",
    "        else:\n",
    "            logger.error(\"Aborting further processing due to previous day's file presence.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main script execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "063f3d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-05-31'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "454c3a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_data_2024-05-30\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97b744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}